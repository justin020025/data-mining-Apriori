{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_in.txt','r',encoding = 'utf8') as Input:\n",
    "    inp = Input.readlines()\n",
    "Input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for line in inp:\n",
    "    sentences.extend(re.split('[.,?]+',re.sub('[!@#$%^&*()\\\\n$:;]','',line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [e.lower().split() for e in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = []\n",
    "with open('stop_words.txt','r',encoding = 'utf8') as s:\n",
    "    stop = re.split('\\n',s.read())\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = []\n",
    "for i in range(len(sentences)):\n",
    "    if sentences[i] == []:\n",
    "        continue\n",
    "    temp = []\n",
    "    for j in range(len(sentences[i])):\n",
    "        if not (sentences[i][j] in stop):\n",
    "            temp.append(sentences[i][j])\n",
    "    if temp != []:        \n",
    "        clean_sentences.append(temp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sublist(lst1, lst2):\n",
    "    return set(lst1).issubset(set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_itemset(table):\n",
    "    itemset = [e[0] for e in table]\n",
    "    Len = len(itemset[0])+1\n",
    "    new_itemset_=[]\n",
    "    for i in range(len(itemset)-1):\n",
    "        for j in range(i+1,len(itemset)):\n",
    "            temp = list(set(itemset[i]).union(set(itemset[j])))\n",
    "            if len(temp) == Len:\n",
    "                if not (temp in new_itemset_):\n",
    "                    new_itemset_.append(temp)\n",
    "        \n",
    "    return new_itemset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_table(data, itemset):\n",
    "    table = [[e,0] for e in itemset]\n",
    "    for i in range(len(itemset)):\n",
    "        for sentence in data:\n",
    "            if is_sublist(itemset[i],sentence):\n",
    "                table[i][1]+=1\n",
    "    return table\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(data, min_sup):\n",
    "    #initialize the itemset\n",
    "    import numpy as np\n",
    "    itemset0  = set()\n",
    "    for sentence in data:\n",
    "        itemset0 = itemset0.union(np.unique(sentence))\n",
    "    itemset0 = [[e] for e in itemset0]\n",
    "    cur_itemset = itemset0\n",
    "    \n",
    "    itemset_maxlen = max([len(sentence) for sentence in data])\n",
    "    \n",
    "    result = []\n",
    "    for i in range(itemset_maxlen-1):\n",
    "        \n",
    "        if len(cur_itemset)>1:\n",
    "            #creat table\n",
    "            cur_table = new_table(data, cur_itemset)\n",
    "            \n",
    "            #delete entry in table if support < min support\n",
    "            clean_table = []\n",
    "            for e in cur_table:\n",
    "                if e[1]>=min_sup:\n",
    "                    clean_table.append(e)\n",
    "            del cur_table\n",
    "            cur_table = clean_table\n",
    "            \n",
    "            #push to result\n",
    "            result.extend(cur_table)\n",
    "    \n",
    "            cur_itemset = new_itemset(cur_table)\n",
    "    \n",
    "    return result\n",
    "            \n",
    "                \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = apriori(clean_sentences, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['observations'], 11],\n",
       " [['additional'], 13],\n",
       " [['maximum'], 12],\n",
       " [['images'], 31],\n",
       " [['expected'], 11],\n",
       " [['specific'], 10],\n",
       " [['ability'], 10],\n",
       " [['optimization'], 46],\n",
       " [['attacks'], 15],\n",
       " [['simple'], 14],\n",
       " [['deep'], 93],\n",
       " [['memory'], 13],\n",
       " [['computing'], 10],\n",
       " [['stochastic'], 19],\n",
       " [['features'], 56],\n",
       " [['output'], 11],\n",
       " [['different'], 45],\n",
       " [['layers'], 14],\n",
       " [['learned'], 16],\n",
       " [['similarity'], 15],\n",
       " [['feature'], 31],\n",
       " [['visual'], 25],\n",
       " [['model'], 125],\n",
       " [['evaluate'], 11],\n",
       " [['energy'], 17],\n",
       " [['challenge'], 27],\n",
       " [['techniques'], 37],\n",
       " [['sample'], 14],\n",
       " [['cases'], 10],\n",
       " [['classifier'], 21],\n",
       " [['leads'], 12],\n",
       " [['labels'], 14],\n",
       " [['effective'], 16],\n",
       " [['better'], 24],\n",
       " [['representation'], 21],\n",
       " [['furthermore'], 11],\n",
       " [['via'], 20],\n",
       " [['despite'], 10],\n",
       " [['analysis'], 44],\n",
       " [['capture'], 10],\n",
       " [['generated'], 10],\n",
       " [['source'], 13],\n",
       " [['future'], 18],\n",
       " [['often'], 18],\n",
       " [['state'], 27],\n",
       " [['space'], 38],\n",
       " [['forecasting'], 10],\n",
       " [['convergence'], 25],\n",
       " [['review'], 11],\n",
       " [['dynamic'], 19],\n",
       " [['obtain'], 14],\n",
       " [['parameters'], 24],\n",
       " [['global'], 16],\n",
       " [['concept'], 10],\n",
       " [['domains'], 15],\n",
       " [['clustering'], 25],\n",
       " [['issue'], 11],\n",
       " [['theory'], 11],\n",
       " [['comparison'], 11],\n",
       " [['rates'], 12],\n",
       " [['learning'], 247],\n",
       " [['approach'], 68],\n",
       " [['network'], 76],\n",
       " [['available'], 23],\n",
       " [['establish'], 11],\n",
       " [['useful'], 14],\n",
       " [['years'], 16],\n",
       " [['tasks'], 41],\n",
       " [['developing'], 10],\n",
       " [['samples'], 23],\n",
       " [['rate'], 19],\n",
       " [['show'], 89],\n",
       " [['0'], 19],\n",
       " [['e'], 24],\n",
       " [['among'], 13],\n",
       " [['best'], 15],\n",
       " [['general'], 15],\n",
       " [['natural'], 10],\n",
       " [['area'], 10],\n",
       " [['tools'], 12],\n",
       " [['even'], 16],\n",
       " [['tensor'], 11],\n",
       " [['sequence'], 13],\n",
       " [['popular'], 11],\n",
       " [['become'], 11],\n",
       " [['recognition'], 12],\n",
       " [['multi-agent'], 13],\n",
       " [['example'], 12],\n",
       " [['gaussian'], 10],\n",
       " [['empirical'], 14],\n",
       " [['uncertainty'], 13],\n",
       " [['experiments'], 38],\n",
       " [['random'], 17],\n",
       " [['setting'], 11],\n",
       " [['standard'], 10],\n",
       " [['learn'], 33],\n",
       " [['provide'], 33],\n",
       " [['estimation'], 23],\n",
       " [['probability'], 13],\n",
       " [['adaptation'], 18],\n",
       " [['still'], 12],\n",
       " [['computational'], 17],\n",
       " [['develop'], 13],\n",
       " [['novel'], 39],\n",
       " [['solution'], 15],\n",
       " [['inference'], 29],\n",
       " [['particular'], 15],\n",
       " [['target'], 21],\n",
       " [['terms'], 19],\n",
       " [['automatically'], 15],\n",
       " [['semantic'], 19],\n",
       " [['due'], 24],\n",
       " [['input'], 18],\n",
       " [['new'], 50],\n",
       " [['approaches'], 24],\n",
       " [['predictive'], 11],\n",
       " [['key'], 13],\n",
       " [['strategy'], 14],\n",
       " [['reinforcement'], 26],\n",
       " [['code'], 17],\n",
       " [['existing'], 28],\n",
       " [['processing'], 11],\n",
       " [['medical'], 17],\n",
       " [['test'], 17],\n",
       " [['machine'], 92],\n",
       " [['across'], 16],\n",
       " [['tuning'], 13],\n",
       " [['processes'], 13],\n",
       " [['provides'], 13],\n",
       " [['three'], 20],\n",
       " [['actions'], 13],\n",
       " [['studies'], 18],\n",
       " [['found'], 10],\n",
       " [['use'], 49],\n",
       " [['adversarial'], 25],\n",
       " [['generalization'], 12],\n",
       " [['find'], 20],\n",
       " [['important'], 20],\n",
       " [['conditions'], 14],\n",
       " [['activity'], 17],\n",
       " [['local'], 28],\n",
       " [['software'], 11],\n",
       " [['application'], 11],\n",
       " [['probabilistic'], 12],\n",
       " [['series'], 14],\n",
       " [['event'], 10],\n",
       " [['two'], 56],\n",
       " [['representations'], 15],\n",
       " [['neural'], 79],\n",
       " [['result'], 27],\n",
       " [['case'], 13],\n",
       " [['allows'], 12],\n",
       " [['high-dimensional'], 11],\n",
       " [['method'], 86],\n",
       " [['applied'], 22],\n",
       " [['efficiency'], 12],\n",
       " [['problem'], 68],\n",
       " [['outperforms'], 15],\n",
       " [['process'], 42],\n",
       " [['propose'], 65],\n",
       " [['g'], 16],\n",
       " [['need'], 16],\n",
       " [['adaptive'], 16],\n",
       " [['networks'], 101],\n",
       " [['constraints'], 12],\n",
       " [['given'], 28],\n",
       " [['significant'], 17],\n",
       " [['communication'], 29],\n",
       " [['various'], 25],\n",
       " [['framework'], 60],\n",
       " [['image'], 12],\n",
       " [['environment'], 18],\n",
       " [['stream'], 13],\n",
       " [['transfer'], 11],\n",
       " [['development'], 15],\n",
       " [['feedback'], 16],\n",
       " [['systems'], 31],\n",
       " [['structure'], 25],\n",
       " [['level'], 10],\n",
       " [['introduce'], 32],\n",
       " [['sparse'], 12],\n",
       " [['well'], 39],\n",
       " [['based'], 71],\n",
       " [['present'], 31],\n",
       " [['multiple'], 28],\n",
       " [['train'], 13],\n",
       " [['first'], 36],\n",
       " [['second'], 11],\n",
       " [['performance'], 81],\n",
       " [['results'], 90],\n",
       " [['paper'], 72],\n",
       " [['convolutional'], 20],\n",
       " [['social'], 11],\n",
       " [['perception'], 10],\n",
       " [['addition'], 16],\n",
       " [['control'], 27],\n",
       " [['high'], 37],\n",
       " [['improved'], 14],\n",
       " [['properties'], 15],\n",
       " [['one'], 34],\n",
       " [['effectiveness'], 14],\n",
       " [['robot'], 13],\n",
       " [['power'], 15],\n",
       " [['dataset'], 27],\n",
       " [['us'], 15],\n",
       " [['real-world'], 25],\n",
       " [['graphs'], 16],\n",
       " [['linear'], 24],\n",
       " [['methods'], 76],\n",
       " [['knowledge'], 28],\n",
       " [['problems'], 42],\n",
       " [['may'], 11],\n",
       " [['variational'], 14],\n",
       " [['statistical'], 14],\n",
       " [['simulation'], 16],\n",
       " [['complex'], 26],\n",
       " [['kernel'], 16],\n",
       " [['research'], 20],\n",
       " [['values'], 11],\n",
       " [['policies'], 19],\n",
       " [['time'], 51],\n",
       " [['study'], 21],\n",
       " [['significantly'], 17],\n",
       " [['solve'], 12],\n",
       " [['strategies'], 16],\n",
       " [['models'], 78],\n",
       " [['training'], 84],\n",
       " [['detect'], 10],\n",
       " [['main'], 11],\n",
       " [['employ'], 10],\n",
       " [['parameter'], 13],\n",
       " [['challenges'], 19],\n",
       " [['online'], 19],\n",
       " [['simulations'], 14],\n",
       " [['literature'], 11],\n",
       " [['autonomous'], 11],\n",
       " [['demonstrate'], 41],\n",
       " [['theoretical'], 18],\n",
       " [['including'], 30],\n",
       " [['design'], 28],\n",
       " [['current'], 24],\n",
       " [['state-of-the-art'], 37],\n",
       " [['benchmark'], 17],\n",
       " [['recently'], 17],\n",
       " [['directly'], 17],\n",
       " [['compression'], 14],\n",
       " [['bayesian'], 25],\n",
       " [['finally'], 16],\n",
       " [['advances'], 12],\n",
       " [['binary'], 12],\n",
       " [['outperform'], 11],\n",
       " [['importance'], 11],\n",
       " [['without'], 27],\n",
       " [['points'], 12],\n",
       " [['dynamics'], 17],\n",
       " [['field'], 12],\n",
       " [['system'], 29],\n",
       " [['action'], 15],\n",
       " [['prior'], 10],\n",
       " [['obtained'], 11],\n",
       " [['label'], 14],\n",
       " [['distributed'], 14],\n",
       " [['object'], 12],\n",
       " [['low'], 14],\n",
       " [['cost'], 13],\n",
       " [['original'], 10],\n",
       " [['amount'], 10],\n",
       " [['prove'], 16],\n",
       " [['achieved'], 12],\n",
       " [['class'], 14],\n",
       " [['address'], 22],\n",
       " [['tensorflow'], 11],\n",
       " [['critical'], 12],\n",
       " [['prediction'], 26],\n",
       " [['sources'], 11],\n",
       " [['structures'], 10],\n",
       " [['flow'], 15],\n",
       " [['several'], 25],\n",
       " [['thus'], 11],\n",
       " [['1'], 18],\n",
       " [['used'], 62],\n",
       " [['recurrent'], 15],\n",
       " [['hyperparameter'], 11],\n",
       " [['required'], 14],\n",
       " [['ml'], 11],\n",
       " [['search'], 21],\n",
       " [['known'], 13],\n",
       " [['matrix'], 17],\n",
       " [['traditional'], 17],\n",
       " [['possible'], 10],\n",
       " [['limited'], 19],\n",
       " [['decision'], 17],\n",
       " [['generation'], 17],\n",
       " [['average'], 11],\n",
       " [['value'], 16],\n",
       " [['experimental'], 19],\n",
       " [['make'], 10],\n",
       " [['speed'], 14],\n",
       " [['policy'], 19],\n",
       " [['product'], 16],\n",
       " [['shown'], 14],\n",
       " [['applications'], 35],\n",
       " [['numerical'], 12],\n",
       " [['enables'], 10],\n",
       " [['number'], 47],\n",
       " [['objective'], 15],\n",
       " [['gradients'], 10],\n",
       " [['previous'], 22],\n",
       " [['achieves'], 16],\n",
       " [['modeling'], 18],\n",
       " [['accuracy'], 33],\n",
       " [['able'], 15],\n",
       " [['discrete'], 12],\n",
       " [['functions'], 21],\n",
       " [['optimal'], 24],\n",
       " [['many'], 29],\n",
       " [['scheme'], 12],\n",
       " [['four'], 11],\n",
       " [['perform'], 23],\n",
       " [['proposed'], 84],\n",
       " [['algorithm'], 75],\n",
       " [['generate'], 13],\n",
       " [['detection'], 31],\n",
       " [['generative'], 18],\n",
       " [['part'], 14],\n",
       " [['insights'], 11],\n",
       " [['distributions'], 16],\n",
       " [['discuss'], 11],\n",
       " [['quality'], 11],\n",
       " [['trained'], 28],\n",
       " [['clusters'], 18],\n",
       " [['layer'], 10],\n",
       " [['developed'], 14],\n",
       " [['consistency'], 10],\n",
       " [['yet'], 10],\n",
       " [['using'], 99],\n",
       " [['architecture'], 18],\n",
       " [['scale'], 11],\n",
       " [['sampling'], 18],\n",
       " [['function'], 46],\n",
       " [['datasets'], 37],\n",
       " [['commonly'], 11],\n",
       " [['hierarchical'], 10],\n",
       " [['work'], 53],\n",
       " [['sequences'], 13],\n",
       " [['recent'], 33],\n",
       " [['domain'], 22],\n",
       " [['synthetic'], 14],\n",
       " [['maps'], 11],\n",
       " [['specifically'], 16],\n",
       " [['however'], 55],\n",
       " [['requires'], 12],\n",
       " [['descent'], 15],\n",
       " [['text'], 15],\n",
       " [['-'], 12],\n",
       " [['distribution'], 22],\n",
       " [['highly'], 11],\n",
       " [['real'], 22],\n",
       " [['loss'], 31],\n",
       " [['classification'], 37],\n",
       " [['language'], 14],\n",
       " [['improve'], 17],\n",
       " [['potential'], 14],\n",
       " [['challenging'], 11],\n",
       " [['imaging'], 11],\n",
       " [['agent'], 23],\n",
       " [['simulated'], 11],\n",
       " [['agents'], 22],\n",
       " [['large-scale'], 13],\n",
       " [['context'], 21],\n",
       " [['gradient'], 29],\n",
       " [['order'], 17],\n",
       " [['programming'], 12],\n",
       " [['efficient'], 27],\n",
       " [['robots'], 12],\n",
       " [['human'], 29],\n",
       " [['algorithms'], 59],\n",
       " [['single'], 15],\n",
       " [['variety'], 12],\n",
       " [['robust'], 13],\n",
       " [['past'], 11],\n",
       " [['typically'], 10],\n",
       " [['large'], 41],\n",
       " [['evaluation'], 11],\n",
       " [['information'], 49],\n",
       " [['graph'], 41],\n",
       " [['data'], 163],\n",
       " [['signal'], 13],\n",
       " [['products'], 11],\n",
       " [['bounds'], 11],\n",
       " [['metrics'], 12],\n",
       " [['compared'], 26],\n",
       " [['task'], 35],\n",
       " [['achieve'], 16],\n",
       " [['regression'], 12],\n",
       " [['scenarios'], 13],\n",
       " [['similar'], 11],\n",
       " [['set'], 32],\n",
       " [['sets'], 16],\n",
       " [['continuous'], 11],\n",
       " [['deep', 'model'], 12],\n",
       " [['deep', 'learning'], 49],\n",
       " [['deep', 'neural'], 22],\n",
       " [['deep', 'networks'], 19],\n",
       " [['learning', 'model'], 18],\n",
       " [['data', 'model'], 11],\n",
       " [['learning', 'techniques'], 10],\n",
       " [['learning', 'show'], 11],\n",
       " [['learning', 'reinforcement'], 25],\n",
       " [['learning', 'machine'], 78],\n",
       " [['learning', 'use'], 11],\n",
       " [['learning', 'neural'], 11],\n",
       " [['framework', 'learning'], 11],\n",
       " [['based', 'learning'], 11],\n",
       " [['learning', 'paper'], 10],\n",
       " [['control', 'learning'], 10],\n",
       " [['learning', 'problems'], 10],\n",
       " [['learning', 'models'], 21],\n",
       " [['learning', 'used'], 10],\n",
       " [['algorithm', 'learning'], 12],\n",
       " [['learning', 'using'], 10],\n",
       " [['domain', 'learning'], 10],\n",
       " [['data', 'learning'], 23],\n",
       " [['network', 'neural'], 26],\n",
       " [['network', 'using'], 12],\n",
       " [['results', 'show'], 24],\n",
       " [['novel', 'propose'], 15],\n",
       " [['existing', 'methods'], 12],\n",
       " [['machine', 'models'], 12],\n",
       " [['networks', 'neural'], 45],\n",
       " [['convolutional', 'neural'], 12],\n",
       " [['neural', 'recurrent'], 10],\n",
       " [['method', 'proposed'], 16],\n",
       " [['networks', 'training'], 10],\n",
       " [['framework', 'proposed'], 10],\n",
       " [['experimental', 'results'], 11],\n",
       " [['proposed', 'results'], 10],\n",
       " [['methods', 'state-of-the-art'], 11],\n",
       " [['data', 'training'], 15],\n",
       " [['algorithm', 'proposed'], 13],\n",
       " [['data', 'proposed'], 13],\n",
       " [['data', 'using'], 11],\n",
       " [['descent', 'gradient'], 12],\n",
       " [['data', 'sets'], 10],\n",
       " [['deep', 'networks', 'neural'], 14],\n",
       " [['deep', 'networks', 'neural'], 14],\n",
       " [['learning', 'machine', 'models'], 12]]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sort = sorted([(sorted(row[0]),row[1]) for row in result], key = lambda x:x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_out.txt','w',encoding = 'utf8') as out:\n",
    "    for e in result_sort:\n",
    "        for word in e[0]:\n",
    "            out.write(word+' ')\n",
    "        out.write(str(e[1]))   \n",
    "        out.write('\\n')\n",
    "out.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
